import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout
import matplotlib.pyplot as plt
from tensorflow.keras.metrics import Precision, Recall, BinaryAccuracy
import cv2
import numpy as np

# Define directories
train_data_dir = "/content/drive/MyDrive/Images/TRAINING SET"
test_data_dir = "/content/drive/MyDrive/Images/TESTING SET"

# Function to load and preprocess data
def load_and_preprocess_data(data_dir, batch_size=32):
    data = tf.keras.preprocessing.image_dataset_from_directory(
        data_dir,
        image_size=(256, 256),
        batch_size=batch_size,
        label_mode='binary'
    )
    data = data.map(lambda x, y: (x / 255.0, y))
    return data

# Load training and validation data
train_val_data = load_and_preprocess_data(train_data_dir)
# Load test data
test_data = load_and_preprocess_data(test_data_dir)

# Check the first batch after normalization
data_iterator = train_val_data.as_numpy_iterator()
batch = data_iterator.next()
print(batch[0].shape)  # Print shape of the batch
print(batch[1])        # Print labels of the batch

# Calculate total size of the training/validation dataset
total_size = tf.data.experimental.cardinality(train_val_data).numpy()
print(f'Total training/validation dataset size: {total_size} batches')

# Split dataset sizes
train_size = int(total_size * 0.8)  # 80% for training
val_size = total_size - train_size  # Remaining for validation

print("-----------------------------------------------------------------------")
print(f'Training set size: {train_size} batches')
print(f'Validation set size: {val_size} batches')
print(f'Test set size: {tf.data.experimental.cardinality(test_data).numpy()} batches')
print("-----------------------------------------------------------------------")

# Split the dataset into training and validation sets
train_data = train_val_data.take(train_size)
val_data = train_val_data.skip(train_size)

# Print final sizes for verification
print(f'Training set size: {tf.data.experimental.cardinality(train_data).numpy()} batches')
print(f'Validation set size: {tf.data.experimental.cardinality(val_data).numpy()} batches')
print(f'Test set size: {tf.data.experimental.cardinality(test_data).numpy()} batches')

# Create the model
model = Sequential()
model.add(Conv2D(16, (3,3), padding='same', activation='relu', input_shape=(256, 256, 3)))
model.add(MaxPooling2D())
model.add(Conv2D(32, (3,3), padding='same', activation='relu'))
model.add(MaxPooling2D())
model.add(Conv2D(16, (3,3), padding='same', activation='relu'))
model.add(MaxPooling2D())
model.add(Flatten())
model.add(Dense(256, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.summary()

# Train the model
logdir = 'drive/MyDrive/logs'
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)
hist = model.fit(train_data, epochs=20, validation_data=val_data, callbacks=[tensorboard_callback])

# Evaluate the model on the test set
test_loss, test_accuracy = model.evaluate(test_data)
print(f'Test Loss: {test_loss}')
print(f'Test Accuracy: {test_accuracy}')

# Retrieve and plot training history
history = hist.history
fig = plt.figure()
plt.plot(history['loss'], color='red', label='Training Loss')
plt.plot(history['val_loss'], color='yellow', label='Validation Loss')
plt.plot(history['accuracy'], color='blue', label='Training Accuracy')
plt.plot(history['val_accuracy'], color='green', label='Validation Accuracy')
plt.title('Model Metrics')
plt.xlabel('Epoch')
plt.ylabel('Metric Value')
plt.legend()
plt.show()

# Validate the model
pre = Precision()
re = Recall()
acc = BinaryAccuracy()
for batch in test_data.as_numpy_iterator():
    X, y = batch
    yhat = model.predict(X)
    pre.update_state(y, yhat)
    re.update_state(y, yhat)
    acc.update_state(y, yhat)

# Print validation metrics
print(f'Precision: {pre.result().numpy()}')
print(f'Recall: {re.result().numpy()}')
print(f'Accuracy: {acc.result().numpy()}')

# Determine class indices
class_names = train_val_data.class_names
print("Class names:", class_names)  # ['NEW', 'OLD']

# Test with a single image
img_path = '/content/drive/MyDrive/Images/TESTING SET/NEW/Copy of t.jpg'  # Path to the image
img = cv2.imread(img_path)
img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
resized_img = tf.image.resize(img_rgb, (256, 256))
plt.imshow(resized_img.numpy().astype(int))
plt.title('Resized Image')
plt.show()

preprocessed_img = resized_img / 255.0
preprocessed_img = np.expand_dims(preprocessed_img, axis=0)
prediction = model.predict(preprocessed_img)

if prediction > 0.5:
    class_label = class_names[1]  # 'OLD'
else:
    class_label = class_names[0]  # 'NEW'

print("Predicted Class:", class_label)
print("Probability:", prediction)
